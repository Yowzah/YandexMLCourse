{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метрики качества классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import f1_score as f_measure\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_task(): #пункт 3 задания\n",
    "    \n",
    "    train = []\n",
    "    with open('classification.csv') as file:\n",
    "        reader = csv.reader(file, delimiter = ',')\n",
    "        for row in reader:\n",
    "            train.append(row)\n",
    "            \n",
    "    train = np.array(train)\n",
    "    \n",
    "    train_true = train[1:,0]\n",
    "    train_true = [int(i) for i in train_true]\n",
    "    \n",
    "    train_pred = train[1:,1]\n",
    "    train_pred = [int(i) for i in train_pred]\n",
    "    \n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    for i in range(len(train_true)):\n",
    "        if train_true[i] == train_pred[i] and train_true[i] == 1:\n",
    "            tp += 1\n",
    "        if train_true[i] == train_pred[i] and train_true[i] == 0:\n",
    "            tn += 1\n",
    "        if train_true[i] != train_pred[i] and train_true[i] == 1:\n",
    "            fn += 1\n",
    "        if train_true[i] != train_pred[i] and train_true[i] == 0:\n",
    "            fp += 1\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Actual positive' : [tp, fp],\n",
    "        'Actual negative' : [tn, fn]\n",
    "    }, index=['Predicted Positive', 'Predicted Negative']);\n",
    "    \n",
    "    #print(df)\n",
    "    \n",
    "    accuracy_result = accuracy(train_true, train_pred) #доля верно угаданных\n",
    "    precision_result = precision(train_true, train_pred) #точность\n",
    "    recall_result = recall(train_true, train_pred) #полнота\n",
    "    f_measure_result = f_measure(train_true, train_pred)\n",
    "    \n",
    "    first_answer = \"Answer: \" + str(\"%.2f\" % accuracy_result) + \" \" + str(\"%.2f\" % precision_result) + \" \" + str(\"%.2f\" % recall_result) +\" \"+ str(\"%.2f\" % f_measure_result)\n",
    "    \n",
    "    print(tp,fp,fn,tn)\n",
    "    \n",
    "    print(first_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 34 59 64\n",
      "Answer: 0.54 0.56 0.42 0.48\n"
     ]
    }
   ],
   "source": [
    "first_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_task(): #пункты 5-6 задания\n",
    "    \n",
    "    train = []\n",
    "    with open('scores.csv') as file:\n",
    "        reader = csv.reader(file, delimiter = ',')\n",
    "        for row in reader:\n",
    "            train.append(row)\n",
    "    train = np.array(train)\n",
    "    \n",
    "    train_true = train[1:,0]\n",
    "    train_logreg = train[1:,1]\n",
    "    train_svm = train[1:,2]\n",
    "    train_knn = train[1:,3]\n",
    "    train_tree = train[1:,4]\n",
    "\n",
    "    train_true = [int(i) for i in train_true]\n",
    "    train_logreg = [float(i) for i in train_logreg]\n",
    "    train_svm = [float(i) for i in train_svm]\n",
    "    train_knn = [float(i) for i in train_knn]\n",
    "    train_tree = [float(i) for i in train_tree]\n",
    "\n",
    "    train = [train_logreg, train_svm, train_knn, train_tree]\n",
    "\n",
    "    result = [['score_logreg',0],['score_svm',0],['score_knn',0],['score_tree',0]]\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        result[i][1] = round(roc(train_true, train[i]),2)\n",
    "    \n",
    "    max = 0\n",
    "    max_i = -1\n",
    "    for i in range(len(result)):\n",
    "        if result[i][1] > max:\n",
    "            max = result[i][1]\n",
    "            max_i = i\n",
    "            \n",
    "    print(result[max_i][0])\n",
    "    \n",
    "    arr = [[],[],[],[]]\n",
    "    max = 0\n",
    "    index = -1\n",
    "    for i in range(len(train)):\n",
    "        arr = precision_recall_curve(train_true, train[i])\n",
    "        m = arr[0][(arr[1] >= 0.7)].max()\n",
    "        if m > max:\n",
    "            max = m\n",
    "            index = i\n",
    "        \n",
    "    print(result[index][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_logreg\n",
      "score_tree\n"
     ]
    }
   ],
   "source": [
    "second_task()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
