\documentclass[a4paper]{article}
\usepackage[14pt]{extsizes} % для того чтобы задать нестандартный 14-ый размер шрифта
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{setspace,amsmath}
\usepackage[left=20mm, top=15mm, right=15mm, bottom=15mm, nohead, footskip=10mm]{geometry} 
\usepackage[pdftex]{graphicx}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\usepackage{amsthm}% настройки полей документа
\lstset{
    language=java,
    upquote=true,
    aboveskip={1.5\baselineskip},
    columns=fullflexible,
    showstringspaces=false,
    extendedchars=true,
    breaklines=true,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    identifierstyle=\ttfamily,
    keywordstyle=\color[rgb]{0,0,1},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\newtheorem{definition}{Определение}
 
\begin{document} % начало документа
 
\begin{titlepage}
  \begin{center}
    \large
    МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ\\ РОССИЙСКОЙ ФЕДЕРАЦИИ
     
    \textbf{Федеральное государственное бюджетное образовательное учреждение высшего профессионального образования}
    \vspace{0.5cm}
 
    САНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ
    \vspace{0.25cm}
     
    Математико-механический факультет
     
    Кафедра прикладной кибернетики
    \vfill
     
     
    Путников Семен Андреевич
    \vfill
 
    \textsc{Курсовая работа}\\[5mm]
     
    {\LARGE Алгоритмы кластеризации: выявление профилей пользователей по анализу активности}
  \bigskip
     
\end{center}
\vfill
 
\newlength{\ML}
\settowidth{\ML}{«\underline{\hspace{0.7cm}}» \underline{\hspace{2cm}}}
\hfill\begin{minipage}{0.4\textwidth}
  Руководитель курсовой работы\\
  \underline{\hspace{\ML}} Н.\,В.~Кузнецов\\
  «\underline{\hspace{0.7cm}}» \underline{\hspace{2cm}} 2019 г.
\end{minipage}%
\bigskip
 

 
\begin{center}
  Санкт-Петербург, 2019 г.
\end{center}
\end{titlepage}
 
\newpage
     
    \tableofcontents % Вывод содержания
\newpage
 
\newpage
\section{Введение}
 Мы живем в постиндустриальном обществе, где как известно: "кто владеет информацией тот владеет миром". Люди стараются собирать сведения обо всем: погоде, экономике, политике, демографии и во многих других отрослях. Каждая компания старается собрать как можно больше данных, надеясь занять лидирующую позицию. Однако сама по себе информация не представляет особой ценности, это своего рода сырье, которое еще нужно обработать. Только после анализа информация представляет для человека или компании определенную ценность, стоит отметить, что чем глубже и точнее произведен анализ информации, тем ценнее его результат. \\
 В рамках анализа и обработки информации появился термин Data Mining, который означает:  добыча данных, интеллектуальный анализ данных, глубинный анализ данных. Достаточно часто Data Mining связывают с Business intelligence, которые в свою очередь широко применяются в бизнесе.  \\
 Задачи, решаемые Data Mining:
 \begin{enumerate} 
\item Классификация — отнесение входного вектора (объекта, события, наблюдения) к одному из заранее известных классов.
\item Кластеризация — разделение множества входных векторов на группы (кластеры) по степени «похожести» друг на друга.
\item Сокращение описания — для визуализации данных, упрощения счета и интерпретации, сжатия объемов собираемой и хранимой информации.
\item Ассоциация — поиск повторяющихся образцов. Например, поиск «устойчивых связей в корзине покупателя».
\item Прогнозирование – нахождение будущих состояний объекта на основании предыдущих состояний (исторических данных)
\item Анализ отклонений — например, выявление нетипичной сетевой активности позволяет обнаружить вредоносные программы.
\item Визуализация данных.
\end{enumerate} 
Рассмотрим задачу кластеризации.  Она является одной из важнейших задач Data Mining. Благодаря ей существует возможноть прогнозировать, а так же получать детальную информацию об уже имеющихся кластерах. Например, в рамках государства это означает, что администрация может оперативно и точно получать информацию о социальных слоях общества, сегментах бизнеса, категориях годности граждан и др., это позволяет аппарату управления быстро реагировать на изменения, а так же задавать вектор развития страны.


\newpage
\section{Постановка задачи}
В данной работе предлагается рассмотреть следующую задачу - анализ имеющихся алгоритмов класстеризации и создание пайплайна для построения класстеризации по массиву данных. Этот массив представляет собой логи REST-запросов пользователей на портале учета древесины и сделок с ней. Проведем декомпозицию задачи и выделим следующие этапы.
\begin{itemize}
  \item Анализ существующих алгоритмов кластеризации
  \item Исследование инструментов Elasticsearch
  \item Обработка массива данных
  \item Реализация алгоритма кластеризации для поставленной задачи
  \item Анализ полученных результатов
\end{itemize}

\newpage
\section{Теоретический аспект}
\subsection{Метрики}
Для работы алгоритмов кластеризации необходимо ввести метрику на данных.
\begin{definition}
	$\rho(x, y): X \times X \rightarrow \mathbb {R}$ называют метрикой, если выполняются следующие условия:
	\begin{enumerate} 
		\item
		$\rho(x, y) = 0 \Leftrightarrow x = y$ (аксиома тождества)
		\item
		$\rho(x, y) = \rho(y, x)$ (аксиома симметрии)
		\item
		$\rho(x, z) \leq \rho(x, y) + \rho(y, z)$ (неравенство треугольника)
	\end{enumerate}
\end{definition}
Приведем наиболее распространенные в Data Mining метрики расстояний.
\begin{enumerate} 
	\item Евклидова метрика
	\begin{equation}
	\rho(x, y) = \sqrt{\sum\limits_{i}^n (x_i - y_i)^2}
	\end{equation}
	\item Квадрат евклидова расстояния
	\begin{equation}
	\rho(x, y) = \sum\limits_{i}^n (x_i - y_i)^2
	\end{equation}
	\item Манхэттенская метрика
	\begin{equation}
	\rho(x, y) = \sum\limits_{i}^n |x_i - y_i|
	\end{equation}
	\item Расстояние Чебышева
	\begin{equation}
	\rho(x, y) = \max_i|x_i - y_i|
	\end{equation}
	\item Степенное расстояние
	\begin{equation}
	\rho(x, y) = \sqrt[r]{\sum\limits_{i}^n (x_i - y_i)^p}
	\end{equation}
	\item Дискретная метрика
	\begin{equation}
	\rho(x, y) = 
	\begin{cases}
	0, x = y,
	\\
	1, x \neq y.
	\end{cases}
	\end{equation}
\end{enumerate} 
 \subsection{Feature engineering}
 Первый этап процесса класстеризации - отбор признаков. Они бывают различных типов: бинарные, вещественные, категориальные, текстовые и другие. В исследуемых нами данных признаки текстовые и категориальные. К категориальным относят признаки, значение которых нельзя сравнивать между собой, можно лишь проверять их равенство. Текстовый признак представляет из себе последовательность слов. Рассмотрим наиболее распространненные алгоритмы их отбора.
 \begin{itemize} 
\item \textbf{One-hot encoding или "Мешок слов".} В рамках этого методы строится словарь всех уникальных слов в датасете, а потом каждому слово приобретает уникальный индекс. Тогда каждое предложение можно будет отобразить списком, длина которого равна числу уникальных слов в словаре, а в каждом индексе в этом списке будет хранится, сколько раз данное слово встречается в предложении. 
\begin{figure}[h!]
	\centering
	\includegraphics[width=100mm]{ucf9sm_gzdytgyripyurqh7fbbe.png}
	\caption{пример работы one-hot encoding.}
	\label{ris:image1}
\end{figure}
\item \textbf{TF-IDF.} В этом методе оценивается не количество вхождений слов, а их оценки важности для текста. Руководствуемся двумя принципами: чем чаще слово встречается в документе, тем оно важнее, и, чем реже слово встречается в остальных документах, тем оно важнее. Рассмотрим формулу на их основе. \\
${n_{iw}}$ (term frequency) - число вхождений слова w в текст ${x_{i}}^j$ \\
$N_{w}$ (document frequency) - число текстов, содержащих $w$ \\
Тогда важность слова $w$ для документа ${x_{i}}^j$ исчисляется по формуле: \\
\begin{equation*}
TF-IDF(i,w)=n_{dw} log(l/N_{w})
\end{equation*}

\end{itemize} 

 \subsection{Класстеризация}
 Класстеризация - это отдельный класс задач машинного обучения, отличиющийся от классификации тем, что у объектов обучающей выборки нет заранее заданных ответов учителя. Задача состоит в выделении отдельных класстеров, состоящих из близких объектов так, чтобы объекты разных класстеров существенно различались. Рассмотрим основные алгоритмы класстеризации.
 \begin{itemize}
 \item \textbf{Метод k-средних.} На входе этого метода некоторым образом расставляются $k$ первоночальных центров класстеров. Потом для каждого объекта вычислется его близость к каждому из класстеров. Затем вычисляется новый центр класстера. И так повторяется пока центры класстеров не стаблизируются.
 \begin{figure}[h!]
 	\centering
 	\includegraphics[width=100mm]{kmeansxmeans.jpg}
 	\caption{пример работы метода k-средних.}
 	\label{ris:image1}
 \end{figure}
 \item \textbf{Алгоритм Ланса-Уильямса.} Этот алгоритм относится к классу агломеративных методов иерархической класстеризации. Агломеративность означает то, что метод объединяет мелкие класстеры в более крупные и отображают историю этих объединений в виде дендрограммы.\\
 
 \begin{figure}[h!]
 	\centering
 	\includegraphics[width=100mm]{image836.jpg}
 	\caption{пример дендрограммы.}
 	\label{ris:image1}
 \end{figure}
 
 Алгоритм основан на том, что сначала мы берём все кластеры одноэлементные,
 то есть каждый кластер соответствует какой-то одной точке,
 и кластеров ровно столько, сколько объектов обучающей выборки.
 А дальше мы находим в текущем множестве кластеров два самых ближайших и
 сливаем их в один кластер. Повторяем это ровно до того момента, пока все кластеры не объединятся в один.\\
 
 Отдельно стоит отметить формулу Ланса-Ульямса, которая используется для нахождения расстояния между кластером $W$, полученным из объединения $U$ и $V$, и кластера S, зная расстояния между исходными кластерами. \\
 \begin{equation*}
 R(U \cup V, S) = \alpha_{U} R(U,S) + \alpha_{V} R(V,S) + \beta R(U,V) + \gamma |R(U,S) - R(V,S)|
 \end{equation*} 
 Частные случаи формулы:
 \begin{enumerate}
 	\item \textbf{Расстояние ближайешего соседа:}
 	\begin{gather*}
 	{R}(W, S) = \min_{w \in W, s \in S} \rho(w,s); \\
 	\text{где $\alpha_{U} = \alpha_{V} = \frac{1}{2}$, $\beta = 0$, $\gamma = -\frac{1}{2}$}
 	\end{gather*}
 	\item \textbf{Расстояние дальнего соседа:}
 	\begin{gather*}
 	{R}(W, S) = \max_{w \in W, s \in S} \rho(w,s); \\
 	\text{где $\alpha_{U} = \alpha_{V} = \frac{1}{2}$, $\beta = 0$, $\gamma = \frac{1}{2}$}
 	\end{gather*}
 	\item \textbf{Расстояние мужду центрами:}
 	\begin{gather*}
 	{R}(W, S) = \rho^2(\sum_{w \in W} \frac{w}{|W|}, \sum_{s \in S} \frac{s}{|S|}); \\
 	\text{где $\alpha_{U} = \frac{|U|}{|W|}$, $\alpha_{V} =\frac{|V|}{|W|}$, $\beta = -\alpha_{U} \alpha_{V}$, $\gamma = 0$}
 	\end{gather*}
 	\item \textbf{Групповое среднее расстояние:}
 	\begin{gather*}
 	{R}(W, S) = \frac{1}{|W||S|}\sum_{w \in W} \sum_{s \in S} \rho (w,s); \\
 	\text{где $\alpha_{U} = \frac{|U|}{|W|}$, $\alpha_{V} =\frac{|V|}{|W|}$, $\beta = \gamma = 0$}
 	\end{gather*}
 \end{enumerate}
 \end{itemize}


\newpage
\section{Практический аспект}
Во время поиска практического решения задачи. Появился ряд проблем связанных с размером массива данных. Входные данные представляют собой JSON-файл весом более 100 гигабайт. На персональных машинах такие объемы не обработать, поэтому был создан пайплайн сборки датасета по кусочкам. \\
Первым делом командой в терминале разбиваем исходный файл на кусочки по 10000 строк и перенесем их в отдельную папку. 
\begin{lstlisting}
mkdir parts && cd parts && split -d -l 10000 --additional-suffix=.json ../data.json ex && cd ..
\end{lstlisting}
Дальше рассмотрим сприпт, который обработает файлы по кусочкам и соберет в один датасет. Обходя дерево json, из первой строки получаем название всех признаков.
\begin{lstlisting}
def getIndexList(dataJson):
	indexSet = set()
	for i in dataJson:
		for k in i.keys():
			if(k != '_source'): indexSet.add(k)
		for j in i['_source'].keys():
			indexSet.add(j)
	indexList = list(indexSet)
	return indexList
\end{lstlisting}
По полученному списку создаем пустой Data Frame. Следущим шагом мы обходим по строчке файл и записываем данные в frame.
\begin{lstlisting}
def writeData(indexList,dataJson,i):
	data = getDataFrame()
	for n in atpbar(range(len(dataJson)), name='LOCAL {}'.format(i)):
		pattern = defaultdict(list)
		for g in indexList: 
			pattern[g].append(None)
		for k in dataJson[n].keys():
			if(k != '_source'): pattern[k] = dataJson[n][k]
		for j in dataJson[n]['_source'].keys(): pattern[j] = dataJson[n]['_source'][j]
		data = data.append(pd.DataFrame.from_dict(pattern), ignore_index=True, sort = False)
	return data
\end{lstlisting}
Как только этот json файл закончится, переходим к следующему файлу и повторяем предыдущие шаги.


\newpage
\section{Итог}
Подводя итог, можно сказать, что цели проекта были частично достигнуты. Была исследованы существующие методы отбора признаков и класстеризации данных, а также их дальнейшей валидации. В результате практической работы был создан скрипт обработки и подготовки данных для применения класстеризации. \\

В процессе работы над проектом были предолены ряд сложностей связанных с размером входных данных. Изучены методы для обработки таких массивов. \\

Проект может иметь продолжение с целями оптимизации алгоритмов класстеризации, а также внедрения данного решения в существующие системы тестирования веб-портала по учету древесины и сделок с ней.

\newpage
%далее сам список используевой литературы
\begin{thebibliography}{}
 \bibitem{litlink3}  Воронцов К.В. Методы кластеризации –  $$http://shad.yandex.ru/lectures/machine_learning.xml$$
 \bibitem{litlink5} Барсегян А. А. и др. Технологии анализа данных: Data Mining, Visual Mining,
Text Mining, OLAP. 2-e изд.‚ перераб. и доп. — СПб.: БХВ-Петербург, 2007.

 \bibitem{litlink6} Вагин B. H., Головина Е. Ю.‚ Загорянская A. A. Достоверный и правдоподобный
вывод в интеллектуальных системах. — М.: Физматлит. 2004.

\bibitem{litlink7}  Кацко И. А., Human H. Б. Практикум по анализу данных на компьютере. — М.:
КолосС, 2009.

\end{thebibliography}

\end{document}  % КОНЕЦ ДОКУМЕНТА !